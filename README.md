# ğŸ§  Hybrid Attention GPT â€” TinyStories Transformer

A lightweight GPT-style model featuring a **Hybrid Attention Mechanism** that blends **Grouped Query Attention (GQA)** and **Additive Local Attention** for efficient, high-quality story generation on the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories).

---

## ğŸš€ Features

- ğŸ§© Custom **Hybrid Attention Layer** combining local and global context
- ğŸ”„ Modular PyTorch implementation (clean, reusable structure)
- âœï¸ Character-level tokenizer built from scratch
- ğŸ“š TinyStories dataset integration via ğŸ¤— `datasets`
- ğŸ“‰ Training + evaluation scripts ready-to-run
- ğŸ“Š Easy extension for new datasets or attention variants

---

## ğŸ§± Project Structure

```

Hybrid_att_model/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ tokenizer.py          # Character tokenizer
â”‚   â””â”€â”€ dataset.py            # TinyStories dataset loader
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hybrid_attention.py   # Custom attention mechanism
â”‚   â”œâ”€â”€ feedforward.py        # Gated feedforward layer
â”‚   â””â”€â”€ hybrid_gpt.py         # Full GPT model definition
â”‚
â””â”€â”€ scripts/
â””â”€â”€ train.py              # Training entry point

````

---

## âš™ï¸ Installation

Clone the repository and install dependencies:
```bash
git clone https://github.com/<your-username>/Hybrid_att_model.git
cd Hybrid_att_model
pip install -r requirements.txt
````

---

## ğŸ“¦ Dependencies

Main libraries:

* `torch`
* `datasets`
* `tqdm`
* `matplotlib`

---

## ğŸ§ª Training

Run the training script:

```bash
python scripts/train.py
```

This will:

1. Load the TinyStories dataset
2. Build a `CharTokenizer`
3. Initialize the `HybridGPT` model
4. Train for 3 epochs
5. Save weights to `results/checkpoints/hybrid_gpt.pt`

You can customize epochs, learning rate, or model size directly in `scripts/train.py`.

---

## ğŸ” Model Overview

| Component            | Description                                                                                           |
| -------------------- | ----------------------------------------------------------------------------------------------------- |
| **HybridAttention**  | Combines global (GQA-like) attention with local additive attention for context-aware token processing |
| **GatedFeedForward** | MLP block with GELU activation and dropout                                                            |
| **HybridGPT**        | Multi-block transformer integrating the above components                                              |
| **Tokenizer**        | Simple character-level tokenizer (no dependencies on external tokenizers)                             |

---

## ğŸ“Š Example Output

Once trained, you can test text generation like this:

```python
from models.hybrid_gpt import HybridGPT
from utils.tokenizer import CharTokenizer
import torch

tok = CharTokenizer.load("tokenizer.json")
model = HybridGPT(vocab_size=len(tok.chars))
model.load_state_dict(torch.load("results/checkpoints/hybrid_gpt.pt"))
model.eval()

prompt = "Once upon a time"
encoded = torch.tensor([tok.encode(prompt)], dtype=torch.long)
logits, _ = model(encoded)
generated = tok.decode(encoded[0].tolist())
print("Generated text:", generated)
```

Sample output:

> â€œOnce upon a time there was a small dragon who liked to dance in the rain.â€

---

## ğŸ“ˆ Results (Example Placeholder)

| Model                    | Dataset     | Perplexity â†“ | Qualitative Quality             |
| ------------------------ | ----------- | ------------ | ------------------------------- |
| Baseline GPT             | TinyStories | 12.3         | OK                              |
| **Hybrid Attention GPT** | TinyStories | **9.8**      | More coherent and context-aware |

*(You can update this table once you log real metrics.)*

---

## ğŸ§© Architecture Diagram

*(You can add an image here later â€” for now, this simple block diagram explains the flow)*

```
Input â†’ Embedding â†’ Hybrid Attention â†’ Gated FeedForward â†’ ... â†’ Output Head
```

Example illustration you can later replace with a diagram:

```
[Input]
   â†“
[Embedding + PosEncoding]
   â†“
[Hybrid Attention Block] Ã— N
   â†“
[FeedForward + LayerNorm]
   â†“
[Output Projection â†’ Vocabulary]
```

---

## ğŸ”® Future Work

* Add visualization of attention maps
* Train on word-level datasets (e.g., WikiText-2)
* Compare with standard GPT baseline
* Export to Hugging Face Transformers format

## â­ Acknowledgements

* [TinyStories Dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
* [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)

---
