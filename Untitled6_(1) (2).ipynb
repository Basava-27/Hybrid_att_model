{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports & Utilities\n",
        "!pip install -q datasets transformers nltk tqdm\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# NLTK tokenizers (download once)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOFFGiFnyG6e",
        "outputId": "e17751af-a9eb-43c2-cbba-09355ebcfaee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Models (cleaned Hybrid + MultiHead + SingleSelfAttention)\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"Grouped Query Self-Attention (cleaned).\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, causal_mask=None, return_attention=False):\n",
        "        # x: (B, T, D)\n",
        "        B, T, _ = x.shape\n",
        "        q = self.W_q(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)  # (B, H, T, d)\n",
        "        k = self.W_k(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
        "        v = self.W_v(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2,-1)) / (self.d_k ** 0.5)  # (B,H,T,T)\n",
        "        if causal_mask is not None:\n",
        "            scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, v)  # (B,H,T,d)\n",
        "        context = context.transpose(1,2).contiguous().view(B, T, self.d_model)\n",
        "        out = self.out(context)\n",
        "        out = self.ln(out + x)\n",
        "        if return_attention:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "\n",
        "class AdditiveLocalAttention(nn.Module):\n",
        "    \"\"\"Additive local attention implemented with batch operations for speed.\"\"\"\n",
        "    def __init__(self, d_model, window_size=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.d_model = d_model\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.v = nn.Parameter(torch.randn(d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        # x: (B, T, D)\n",
        "        B, T, D = x.shape\n",
        "        q_all = self.W_q(x)  # (B, T, D)\n",
        "\n",
        "        outputs = []\n",
        "        attn_maps = []\n",
        "        # Efficiently process by sliding windows\n",
        "        for i in range(T):\n",
        "            start = max(0, i - self.window_size)\n",
        "            local = x[:, start:i+1, :]      # (B, L, D)\n",
        "            q = q_all[:, i, :].unsqueeze(1) # (B, 1, D)\n",
        "            keys = self.W_k(local)          # (B, L, D)\n",
        "\n",
        "            # Additive score: tanh(q + k) Â· v\n",
        "            # expand q to (B, L, D)\n",
        "            scores = torch.tanh(q + keys)   # (B, L, D)\n",
        "            scores = torch.matmul(scores, self.v)  # (B, L)\n",
        "            attn = torch.softmax(scores, dim=-1)   # (B, L)\n",
        "            attn = self.dropout(attn)\n",
        "            weighted = torch.sum(attn.unsqueeze(-1) * local, dim=1)  # (B, D)\n",
        "            outputs.append(weighted)\n",
        "            attn_maps.append(attn)\n",
        "\n",
        "        out = torch.stack(outputs, dim=1)  # (B, T, D)\n",
        "        out = self.ln(out + x)\n",
        "        if return_attention:\n",
        "            return out, attn_maps\n",
        "        return out\n",
        "\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model*2, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, global_out, local_out):\n",
        "        # both: (B, T, D)\n",
        "        cat = torch.cat([global_out, local_out], dim=-1)\n",
        "        gate = self.mlp(cat)  # (B, T, 1)\n",
        "        return gate * global_out + (1 - gate) * local_out\n",
        "\n",
        "\n",
        "class HybridAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, window_size=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.global_attn = GroupedQueryAttention(d_model, num_heads, dropout)\n",
        "        self.local_attn = AdditiveLocalAttention(d_model, window_size, dropout)\n",
        "        self.gate = GatingMechanism(d_model, dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, 4*d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4*d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, causal_mask=None, return_attention=False):\n",
        "        if return_attention:\n",
        "            g_out, g_attn = self.global_attn(x, causal_mask=causal_mask, return_attention=True)\n",
        "            l_out, l_attn = self.local_attn(x, return_attention=True)\n",
        "        else:\n",
        "            g_out = self.global_attn(x, causal_mask=causal_mask)\n",
        "            l_out = self.local_attn(x)\n",
        "\n",
        "        fused = self.gate(g_out, l_out)\n",
        "        ffn_out = self.ffn(fused)\n",
        "        out = self.ln(ffn_out + fused)\n",
        "        if return_attention:\n",
        "            return out, (g_attn, l_attn)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HybridAttentionLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, window_size=32, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        self.layers = nn.ModuleList([HybridAttentionBlock(d_model, num_heads, window_size, dropout) for _ in range(num_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)\n",
        "        h = self.embedding(x) + self.pos_embedding(pos)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        attention_maps = []\n",
        "        for layer in self.layers:\n",
        "            if return_attention:\n",
        "                h, attn = layer(h, causal_mask=causal_mask, return_attention=True)\n",
        "                attention_maps.append(attn)\n",
        "            else:\n",
        "                h = layer(h, causal_mask=causal_mask)\n",
        "        h = self.ln(h)\n",
        "        logits = self.output(h)\n",
        "        if return_attention:\n",
        "            return logits, attention_maps\n",
        "        return logits\n",
        "\n",
        "    def get_num_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Baselines\n",
        "class MultiHeadModel(nn.Module):\n",
        "    \"\"\"Standard Transformer encoder-style autoregressive model using nn.TransformerEncoderLayer\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model, nhead=num_heads, dim_feedforward=4*d_model, dropout=dropout, activation='gelu', batch_first=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)\n",
        "        h = self.embedding(x) + self.pos_embedding(pos)\n",
        "        h = self.dropout(h)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, src_mask=mask)\n",
        "        h = self.ln(h)\n",
        "        return self.output(h)\n",
        "\n",
        "\n",
        "class SingleSelfAttentionModel(nn.Module):\n",
        "    \"\"\"Single-head self attention (no multi-head splitting) + feed-forward layers\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=512, num_layers=6, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(d_model),\n",
        "                nn.Linear(d_model, 4*d_model),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*d_model, d_model)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)\n",
        "        h = self.embedding(x) + self.pos_embedding(pos)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        q = self.W_q(h)  # (B,T,D)\n",
        "        k = self.W_k(h)\n",
        "        v = self.W_v(h)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (h.size(-1) ** 0.5)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        h = torch.matmul(attn, v)  # (B, T, D)\n",
        "        for blk in self.blocks:\n",
        "            h = h + blk(h)\n",
        "        h = self.ln(h)\n",
        "        return self.output(h)\n"
      ],
      "metadata": {
        "id": "MnippDQJyRli"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Dataset & DataLoader helpers\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "        print(f\"Tokenizing {len(texts)} samples...\")\n",
        "        for item in tqdm(texts):\n",
        "            txt = item['text'].strip()\n",
        "            if not txt:\n",
        "                continue\n",
        "            tokens = tokenizer(txt, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n",
        "            self.examples.append(tokens['input_ids'].squeeze(0))\n",
        "        print(\"Done tokenizing.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def prepare_data(max_seq_length=256, use_small_dataset=False):\n",
        "    if use_small_dataset:\n",
        "        ds = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "    else:\n",
        "        ds = load_dataset('wikitext', 'wikitext-103-v1')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    train_ds = WikiTextDataset(ds['train'], tokenizer, max_seq_length)\n",
        "    val_ds = WikiTextDataset(ds['validation'], tokenizer, max_seq_length)\n",
        "    test_ds = WikiTextDataset(ds['test'], tokenizer, max_seq_length)\n",
        "\n",
        "    print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}, Vocab: {vocab_size}\")\n",
        "    return train_ds, val_ds, test_ds, tokenizer, vocab_size\n"
      ],
      "metadata": {
        "id": "w8bEZP4EyZKL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 (fixed): Training & Evaluation with reshape instead of view\n",
        "\n",
        "def compute_causal_mask(T, device):\n",
        "    return torch.triu(torch.ones(T, T, device=device), diagonal=1).bool()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, scaler, criterion, device,\n",
        "                vocab_size, gradient_accumulation_steps=1, max_grad_norm=1.0, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Train E{epoch+1}\")\n",
        "\n",
        "    for i, batch in pbar:\n",
        "        batch = batch.to(device)\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:]\n",
        "\n",
        "        # Use new autocast API\n",
        "        with torch.amp.autocast(device_type=device.type):\n",
        "            logits = model(inputs)\n",
        "            # ðŸ”¥ FIXED: reshape instead of view\n",
        "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i+1) % gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * gradient_accumulation_steps\n",
        "        pbar.set_postfix({'loss': f\"{(total_loss/(i+1)):.4f}\"})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, vocab_size):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            batch = batch.to(device)\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # ðŸ”¥ FIXED: reshape instead of view\n",
        "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = float(torch.exp(torch.tensor(avg_loss)))\n",
        "    return avg_loss, perplexity\n"
      ],
      "metadata": {
        "id": "OVgteIl5ye0B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Generation & Metrics\n",
        "\n",
        "def generate_text(model, tokenizer, prompt: str, device, max_length=50, temperature=0.8, top_k=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    generated = input_ids\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            logits = model(generated)\n",
        "            next_token_logits = logits[0, -1, :] / max(temperature, 1e-8)\n",
        "            if top_k > 0:\n",
        "                vals, idx = torch.topk(next_token_logits, top_k)\n",
        "                min_topk = vals[-1]\n",
        "                next_token_logits[next_token_logits < min_topk] = float('-inf')\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def compute_bleu_for_generations(model, tokenizer, prompts: List[str], device, max_length=50):\n",
        "    scores = []\n",
        "    for p in prompts:\n",
        "        gen = generate_text(model, tokenizer, p, device, max_length=max_length)\n",
        "        # tokenize using tokenizer.tokenize\n",
        "        ref_tokens = tokenizer.tokenize(p)\n",
        "        hyp_tokens = tokenizer.tokenize(gen)\n",
        "        # handle empty or too-short\n",
        "        if len(hyp_tokens) == 0:\n",
        "            scores.append(0.0)\n",
        "        else:\n",
        "            try:\n",
        "                score = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5, 0, 0))\n",
        "            except Exception:\n",
        "                score = 0.0\n",
        "            scores.append(score)\n",
        "    return float(np.mean(scores))\n"
      ],
      "metadata": {
        "id": "rpexp4F_ykB3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Comparison pipeline (train or load each model and compare)\n",
        "\n",
        "def compare_models(hybrid_model, multi_model, self_model,\n",
        "                   tokenizer, test_loader, device, vocab_size):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # Evaluate losses & perplexities\n",
        "    h_loss, h_ppl = evaluate(hybrid_model, test_loader, criterion, device, vocab_size)\n",
        "    m_loss, m_ppl = evaluate(multi_model, test_loader, criterion, device, vocab_size)\n",
        "    s_loss, s_ppl = evaluate(self_model, test_loader, criterion, device, vocab_size)\n",
        "\n",
        "    # Example prompts for BLEU (you can replace with a batch of test prompts)\n",
        "    sample_prompts = [\n",
        "        \"The history of artificial intelligence began\",\n",
        "        \"In the year 2020 the global economy\",\n",
        "        \"Quantum computing has the potential to\",\n",
        "        \"The quick brown fox jumps over the lazy dog\"\n",
        "    ]\n",
        "\n",
        "    print(\"Generating texts for BLEU evaluation (may be slow)...\")\n",
        "    h_bleu = compute_bleu_for_generations(hybrid_model, tokenizer, sample_prompts, device)\n",
        "    m_bleu = compute_bleu_for_generations(multi_model, tokenizer, sample_prompts, device)\n",
        "    s_bleu = compute_bleu_for_generations(self_model, tokenizer, sample_prompts, device)\n",
        "\n",
        "    results = {\n",
        "        \"Hybrid\": {\"loss\": h_loss, \"ppl\": h_ppl, \"bleu\": h_bleu},\n",
        "        \"MultiHead\": {\"loss\": m_loss, \"ppl\": m_ppl, \"bleu\": m_bleu},\n",
        "        \"SelfAttention\": {\"loss\": s_loss, \"ppl\": s_ppl, \"bleu\": s_bleu}\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Comparison Results ===\")\n",
        "    print(\"{:<12} {:>10} {:>12} {:>10}\".format(\"Model\", \"Loss\", \"Perplexity\", \"BLEU\"))\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k:<12} {v['loss']:10.4f} {v['ppl']:12.2f} {v['bleu']:10.4f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "khgtfFCUynF5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Demo run (small dataset + quick training + compare)\n",
        "# Adjust parameters for a real run.\n",
        "\n",
        "# Prepare small data for demo\n",
        "train_ds, val_ds, test_ds, tokenizer, vocab_size = prepare_data(max_seq_length=128, use_small_dataset=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize models (small sizes for demo)\n",
        "hybrid = HybridAttentionLanguageModel(vocab_size=vocab_size, d_model=256, num_heads=4, num_layers=3, window_size=16, max_seq_len=128, dropout=0.1).to(device)\n",
        "multi = MultiHeadModel(vocab_size=vocab_size, d_model=256, num_heads=4, num_layers=3, max_seq_len=128).to(device)\n",
        "single = SingleSelfAttentionModel(vocab_size=vocab_size, d_model=256, num_layers=3, max_seq_len=128).to(device)\n",
        "\n",
        "print(\"Params: Hybrid\", hybrid.get_num_parameters(), \"Multi\", sum(p.numel() for p in multi.parameters() if p.requires_grad), \"Single\", sum(p.numel() for p in single.parameters() if p.requires_grad))\n",
        "\n",
        "# Training setup (very small/fast demo)\n",
        "def quick_train(model, train_loader, val_loader, epochs=5, lr=3e-4):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    steps_per_epoch = max(1, len(train_loader))\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "    scaler = GradScaler()\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    for e in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, device, vocab_size, gradient_accumulation_steps=1, max_grad_norm=1.0, epoch=e)\n",
        "        val_loss, val_ppl = evaluate(model, val_loader, criterion, device, vocab_size)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        print(f\"Epoch {e+1}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_ppl={val_ppl:.2f}\")\n",
        "    return history\n",
        "\n",
        "# Quick train each model for 1 epoch (demo)\n",
        "print(\"Training Hybrid (demo)...\")\n",
        "quick_train(hybrid, train_loader, val_loader, epochs=1)\n",
        "print(\"Training Multi (demo)...\")\n",
        "quick_train(multi, train_loader, val_loader, epochs=1)\n",
        "print(\"Training Single (demo)...\")\n",
        "quick_train(single, train_loader, val_loader, epochs=1)\n",
        "\n",
        "# Compare (evaluation + BLEU on sample prompts)\n",
        "results = compare_models(hybrid, multi, single, tokenizer, test_loader, device, vocab_size)\n",
        "\n",
        "# Save results\n",
        "with open('model_comparison_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Demo complete. Results saved to model_comparison_results.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaL7gbLAytXO",
        "outputId": "2127a2f8-9c29-476f-e6c2-934efa8a6395"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing 36718 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:18<00:00, 2033.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Tokenizing 3760 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:01<00:00, 2201.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Tokenizing 4358 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:02<00:00, 2149.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Train: 23767, Val: 2461, Test: 2891, Vocab: 50257\n",
            "Params: Hybrid 28976212 Multi 28184401 Single 27590737\n",
            "Training Hybrid (demo)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3524012815.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Train E1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1486/1486 [10:58<00:00,  2.26it/s, loss=6.9006]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154/154 [00:18<00:00,  8.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=6.9006 val_loss=6.2219 val_ppl=503.66\n",
            "Training Multi (demo)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train E1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1486/1486 [01:15<00:00, 19.59it/s, loss=7.2600]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154/154 [00:04<00:00, 36.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=7.2600 val_loss=6.6935 val_ppl=807.13\n",
            "Training Single (demo)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train E1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1486/1486 [01:08<00:00, 21.72it/s, loss=7.3106]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154/154 [00:03<00:00, 38.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=7.3106 val_loss=6.8528 val_ppl=946.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:22<00:00,  8.22it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:04<00:00, 36.89it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:04<00:00, 39.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating texts for BLEU evaluation (may be slow)...\n",
            "\n",
            "=== Comparison Results ===\n",
            "Model              Loss   Perplexity       BLEU\n",
            "Hybrid           6.2335       509.54     0.1183\n",
            "MultiHead        6.7240       832.17     0.1192\n",
            "SelfAttention     6.8899       982.32     0.1183\n",
            "Demo complete. Results saved to model_comparison_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= SECTION A ==========================\n",
        "# 10-Epoch Full Training Loop for All Models\n",
        "# =============================================================\n",
        "\n",
        "def train_full(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    tokenizer,\n",
        "    vocab_size,\n",
        "    epochs=10,\n",
        "    lr=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_grad_norm=1.0,\n",
        "    save_dir=\"checkpoints_model\"\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    total_steps = (len(train_loader) // gradient_accumulation_steps) * epochs\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=(total_steps - warmup_steps)\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    scaler = torch.amp.GradScaler()  # FIXED\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"val_ppl\": [], \"lr\": []}\n",
        "\n",
        "    print(\"\\nðŸš€ Starting Training\\n\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n======================= Epoch {epoch+1}/{epochs} =======================\")\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "        for i, batch in enumerate(pbar):\n",
        "            batch = batch.to(device)\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            with torch.amp.autocast(device_type=('cuda' if device.type=='cuda' else 'cpu')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            total_train_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "            # Gradient update\n",
        "            if (i+1) % gradient_accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Warmup LR\n",
        "                if global_step < warmup_steps:\n",
        "                    optimizer.param_groups[0]['lr'] = lr * global_step / warmup_steps\n",
        "                else:\n",
        "                    scheduler.step()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{total_train_loss/(i+1):.4f}\",\n",
        "                \"lr\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "        # Epoch summary\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "        val_loss, val_ppl = evaluate(model, val_loader, criterion, device, vocab_size)\n",
        "\n",
        "        print(f\"\\nðŸ“Š Epoch {epoch+1} Results\")\n",
        "        print(f\"   Train Loss     : {train_loss:.4f}\")\n",
        "        print(f\"   Val Loss       : {val_loss:.4f}\")\n",
        "        print(f\"   Val Perplexity : {val_ppl:.2f}\")\n",
        "\n",
        "        # Save losses\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_ppl\"].append(val_ppl)\n",
        "        history[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Save best checkpoint\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/best.pt\")\n",
        "            print(\"   âœ… Saved best model\")\n",
        "\n",
        "        # Save epoch checkpoint\n",
        "        torch.save(model.state_dict(), f\"{save_dir}/epoch_{epoch+1}.pt\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ TRAINING COMPLETE\\n\")\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "2WJnYOE87Imq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= SECTION B ==========================\n",
        "# Dataset + Model Initialization\n",
        "# =============================================================\n",
        "\n",
        "train_ds, val_ds, test_ds, tokenizer, vocab_size = prepare_data(\n",
        "    max_seq_length=256,\n",
        "    use_small_dataset=True  # set False for real training\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=4)\n",
        "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "print(\"Dataset Ready!\")\n",
        "\n",
        "# Build all 3 models\n",
        "hybrid_model = HybridAttentionLanguageModel(vocab_size, 512, 8, 6, 32, 256).to(device)\n",
        "multi_model  = MultiHeadModel(vocab_size, 512, 8, 6, 256).to(device)\n",
        "single_model = SingleSelfAttentionModel(vocab_size, 512, 6, 256).to(device)\n",
        "\n",
        "print(\"All models initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjTW15Y07MES",
        "outputId": "2e2c851a-74d7-4124-e506-dd5a6a979658"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing 36718 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:25<00:00, 1434.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Tokenizing 3760 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:02<00:00, 1602.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Tokenizing 4358 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:02<00:00, 1596.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing.\n",
            "Train: 23767, Val: 2461, Test: 2891, Vocab: 50257\n",
            "Dataset Ready!\n",
            "All models initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= SECTION C ==========================\n",
        "# TRAIN ALL 3 MODELS\n",
        "# =============================================================\n",
        "\n",
        "hybrid_dir = \"hybrid_10epoch\"\n",
        "multi_dir  = \"multi_10epoch\"\n",
        "single_dir = \"single_10epoch\"\n",
        "\n",
        "print(\"\\n=== Training Hybrid Model ===\")\n",
        "train_full(hybrid_model, train_loader, val_loader, tokenizer, vocab_size,\n",
        "           epochs=10, save_dir=hybrid_dir)\n",
        "\n",
        "print(\"\\n=== Training Multi-Head Transformer ===\")\n",
        "train_full(multi_model, train_loader, val_loader, tokenizer, vocab_size,\n",
        "           epochs=10, save_dir=multi_dir)\n",
        "\n",
        "print(\"\\n=== Training Single Self-Attention ===\")\n",
        "train_full(single_model, train_loader, val_loader, tokenizer, vocab_size,\n",
        "           epochs=10, save_dir=single_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOb-X7sX7OzS",
        "outputId": "c942f551-c5b3-4691-bb02-b9f7691afbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Hybrid Model ===\n",
            "\n",
            "ðŸš€ Starting Training\n",
            "\n",
            "\n",
            "======================= Epoch 1/10 =======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|â–         | 73/1486 [02:40<47:56,  2.04s/it, loss=10.2900, lr=9.42e-6]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= SECTION D ==========================\n",
        "# Evaluation + BLEU Wrapper Function\n",
        "# =============================================================\n",
        "\n",
        "def evaluate_dir(model_builder, ckpt_dir, name):\n",
        "    print(f\"\\n=== Evaluating {name} ===\")\n",
        "\n",
        "    model = model_builder().to(device)\n",
        "    model.load_state_dict(torch.load(f\"{ckpt_dir}/best.pt\"))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    test_loss, test_ppl = evaluate(model, test_loader, criterion, device, vocab_size)\n",
        "\n",
        "    bleu = compute_bleu_for_generations(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompts=[\n",
        "            \"The history of artificial intelligence began\",\n",
        "            \"Quantum computing has the potential to\",\n",
        "            \"In the future, machine learning models will\",\n",
        "        ],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return {\"loss\": test_loss, \"ppl\": test_ppl, \"bleu\": bleu}\n",
        "\n",
        "def build_hybrid():\n",
        "    return HybridAttentionLanguageModel(vocab_size, 512, 8, 6, 32, 256)\n",
        "\n",
        "def build_multi():\n",
        "    return MultiHeadModel(vocab_size, 512, 8, 6, 256)\n",
        "\n",
        "def build_single():\n",
        "    return SingleSelfAttentionModel(vocab_size, 512, 6, 256)\n"
      ],
      "metadata": {
        "id": "9r3NTFsfAX7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= SECTION E ==========================\n",
        "# Final Comparison Table\n",
        "# =============================================================\n",
        "\n",
        "results_hybrid = evaluate_dir(build_hybrid, hybrid_dir, \"HYBRID\")\n",
        "results_multi  = evaluate_dir(build_multi,  multi_dir,  \"MULTI-HEAD\")\n",
        "results_single = evaluate_dir(build_single, single_dir, \"SINGLE-HEAD\")\n",
        "\n",
        "print(\"\\n================== FINAL MODEL COMPARISON ==================\")\n",
        "print(\"{:<15} {:<12} {:<12} {:<10}\".format(\"Model\", \"Loss\", \"Perplexity\", \"BLEU\"))\n",
        "\n",
        "print(\"{:<15} {:<12.4f} {:<12.2f} {:<10.4f}\".format(\n",
        "    \"Hybrid\", results_hybrid[\"loss\"], results_hybrid[\"ppl\"], results_hybrid[\"bleu\"]\n",
        "))\n",
        "\n",
        "print(\"{:<15} {:<12.4f} {:<12.2f} {:<10.4f}\".format(\n",
        "    \"MultiHead\", results_multi[\"loss\"], results_multi[\"ppl\"], results_multi[\"bleu\"]\n",
        "))\n",
        "\n",
        "print(\"{:<15} {:<12.4f} {:<12.2f} {:<10.4f}\".format(\n",
        "    \"SelfAttention\", results_single[\"loss\"], results_single[\"ppl\"], results_single[\"bleu\"]\n",
        "))\n",
        "print(\"===========================================================\\n\")\n"
      ],
      "metadata": {
        "id": "iEc_0W9GAa96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}